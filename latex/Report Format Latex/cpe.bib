

@misc{a,
  author = {K. Fosu},
  title = {How Unintentional Repetition Hurts Your Writing},
  howpublished = "Available at \url{https://medium.com/the-brave-writer/how-repetition-hurts-your-writing-cb68c292525f}",
  year = {2020}, 
  note = "[Online; accessed October 2022]"
}


@article{ b,
  author =	 {J. Piskorski and N. Stefanovitch and G. Jacquet and A. Podavini},
  title =	 {Exploring Linguistically-Lightweight Keyword Extraction Techniques for Indexing News Articles in a Multilingual Set-up},
  journal =	 {Aclanthology},
}

@misc{c,
  author = {IBM},
  title = {Natural Language Processing (NLP)},
  howpublished = "Available at \url{https://www.ibm.com/cloud/learn/natural-language-processing}" ,
  year = {2020}, 
  note = "[Online; accessed October 2022]"
}

@misc{d,
  author = {R. Horev},
  title = {BERT Explained: State of the art language model for NLP},
  howpublished = "Available at \url{https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270}",
  year = {2018}, 
  note = "[Online; accessed October 2022]"
}

@misc{e,
  author = {D. Jurafsky and J.H. Martin},
  title = { Vector Semantics and Embeddings},
  howpublished = "Available at \url{https://web.stanford.edu/~jurafsky/slp3/6.pdf}",
  year = {2021}, 
  note = "[Online; accessed October 2022]"
}

@misc{f,
  author = {T. Pradeep},
  title = {  Keyword Extraction Methods from Document in NLP },
  howpublished = "Available at \url{https://www.analyticsvidhya.com/blog/2022/03/keyword-extraction-methods-from-documents-in-nlp/}",
  year = {2022}, 
  note = "[Online; accessed October 2022]"
}

@misc{g,
  author = {Prowritingaid},
  title = {  What makes ProWritingAid different? },
  howpublished = "Available at \url{https://prowritingaid.com/}",
  note = "[Online; accessed October 2022]"
}
@misc{h,
  author = {Grammarly},
  title = {  About Grammarly },
  howpublished = "Available at \url{https://www.grammarly.com/about}",
  note = "[Online; accessed October 2022]"
}

@misc{i,
  author = {Writer.com},
  title = {  Product Overview },
  howpublished = "Available at \url{https://writer.com/product/overview/}",
  note = "[Online; accessed October 2022]"
}



@Article{j,
  author = 	 {J. Devlin and M. Chang and K. Lee and K. Toutanova},
  title = 	 {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  journal = 	 arXiv,
  year = 	 2019,
}

@Article{k,
  author = 	 {W. Zhou and T. Ge and W. Xu and W. Faru and Z. Ming},
  title = 	 { BERT-based Lexical Substitution},
  journal = 	 {ACL Anthology},

}

@Article{l,
  author = 	 {J. Qiang and Y. Li and Y. Zhu},
  title = 	 {A Simple BERT-Based Approach for Lexical Simplification},
  journal = 	 {arXiv},
}

@misc{m,
  author = {U. Ankit},
  title = {  Transformer Neural Networks: A Step-by-Step Breakdown },
  howpublished = "Available at \url{https://builtin.com/artificial-intelligence/transformer-neural-network}",
  year = {2022}, 
  note = "[Online; accessed November 2022]"
}

@misc{n,
  author = {Maxime},
  title = {  What is a Transformer?
 },
  howpublished = "Available at \url{https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04}",
  year = {2019}, 
  note = "[Online; accessed November 2022]"
}

@Article{o,
  author = 	 {N. Arefyev and B. Sheludko and A. Podolskiy and A. Panchenko},
  title = 	 {Always Keep your Target in Mind: Studying Semantics and
Improving Performance of Neural Lexical Substitution},
  journal = 	 {ACL Anthology},
}

@Article{p,
  author = 	 {A. Vaswani and N. Shazeer and N. Parmar and J. Uszkoreit and L. Jones and A. N. Gomez and L. KKaiser},
  title = 	 {Attention Is All You Need},
  journal = 	 {arXiv},
}








